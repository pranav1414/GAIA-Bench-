# Model Evaluation Pipeline with Streamlit and OpenAI

Project Overview

This project aims to develop a Streamlit-based model evaluation pipeline that allows users to evaluate an OpenAI model's performance against predefined test cases from the GAIA dataset. The pipeline facilitates interactive model testing by letting users select specific test cases, pose questions to the OpenAI model, and compare the model's answers against the correct responses stored in the dataset. If the model's answer is incorrect, users can send the context and annotation steps back to the model for re-evaluation, promoting an iterative process for improving accuracy.
