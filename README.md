# Model Evaluation Pipeline with Streamlit and OpenAI

**Project Overview :**

This project aims to develop a Streamlit-based model evaluation pipeline that allows users to evaluate an OpenAI model's performance against predefined test cases from the GAIA dataset. The pipeline facilitates interactive model testing by letting users select specific test cases, pose questions to the OpenAI model, and compare the model's answers against the correct responses stored in the dataset. If the model's answer is incorrect, users can send the context and annotation steps back to the model for re-evaluation, promoting an iterative process for improving accuracy.

**Key Technologies :**

Google Cloud Platform, Streamlit, OpenAI, VS code, CodeLabs, Git, Python

**Desired Outcome or Solution :**

User-Friendly Interface 

Integration with Streamlit

Integration with OpenAI

Comparison of Values

Feedback and Modification Options

**Contribution :**

WE ATTEST THAT WE HAVEN’T USED ANY OTHER STUDENTS’ WORK IN OUR 
ASSIGNMENT AND ABIDE BY THE POLICIES LISTED IN THE STUDENT HANDBOOK

| Name            | Contribution %                       |
|------------------|-------------------------------------|
| Shubham Agarwal  | 33.33 %                             |
| Chinmay Sawant   | 33.33 %                             |
| Pranav Sonje     | 33.33 %                             |
